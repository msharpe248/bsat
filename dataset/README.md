# SAT Solver Test Datasets

This directory contains various SAT instance datasets used for benchmarking and testing.

## Datasets

### Generated Test Suites

These test suites are **generated using scripts** in `../research/benchmarks/`:

#### Medium Tests
- **Generated by**: `../research/benchmarks/generate_medium_tests.py`
- **Size**: 53 instances
- **Difficulty**: 10-120 variables, ranging from easy to moderately hard
- **Purpose**: Primary benchmarking suite showing meaningful performance differences
- **Regenerate**: `cd ../research/benchmarks && ./generate_medium_tests.py`

#### Simple Tests
- **Generated by**: `../research/benchmarks/generate_simple_tests.py`
- **Size**: 9 instances
- **Difficulty**: 5-30 variables, very easy
- **Purpose**: Quick validation that solvers work correctly
- **Regenerate**: `cd ../research/benchmarks && ./generate_simple_tests.py`

### Competition Benchmarks

#### SAT Competition 2025
- **Source**: https://satcompetition.github.io/2025/
- **Details**: See `sat_competition2025/README.md`
- **Size**: 399 instances (families organized by subdirectories)
- **Difficulty**: Extremely challenging - designed for industrial solvers
- **Warning**: Most instances will timeout on basic implementations

#### SAT Competition 2025 Small Subset
- **Generated by**: `../research/benchmarks/create_small_test_set.py`
- **Source**: Filtered from SAT Competition 2025
- **Size**: 8 instances (≤ 500 variables, ≤ 2000 clauses, ≤ 0.5 MB)
- **Difficulty**: Still very hard, most will timeout
- **Purpose**: Smallest competition instances for testing limits
- **Regenerate**: `cd ../research/benchmarks && ./create_small_test_set.py`

## Results

### Medium Suite Results (October 2025)
- **Location**: `results/medium_suite_results_oct_2025/`
- **Benchmark**: 53 medium-difficulty instances
- **Solvers**: DPLL, CDCL, CoBD-SAT, BB-CDCL, LA-CDCL, CGPM-SAT
- **Winner**: CGPM-SAT (25 wins, 47% win rate, 0 timeouts)
- **Details**: See `../research/benchmarks/BENCHMARK_RESULTS.md`

## Directory Structure

```
dataset/
├── README.md                          # This file
├── simple_tests/                      # Generated: 9 easy instances
│   └── simple_suite/
│       └── *.cnf
├── medium_tests/                      # Generated: 53 medium instances
│   └── medium_suite/
│       └── *.cnf
├── sat_competition2025/               # Downloaded: 399 competition instances
│   ├── README.md
│   ├── algorithm-equivalence-checking/
│   ├── argumentation/
│   ├── ...                            # 99 family directories
│   └── *.cnf files
├── sat_competition2025_small/         # Filtered: 8 smallest competition instances
│   └── */
│       └── *.cnf
└── results/
    └── medium_suite_results_oct_2025/ # Benchmark results (October 2025)
        ├── master_summary.csv
        ├── overall_rankings.txt
        ├── medium_suite.results.txt
        └── medium_suite.summary.txt
```

## Notes

- **CNF files are not checked into git** (too large, can be regenerated)
- **Directory structure is checked in** (with README files)
- **Named result directories are checked in** (e.g., medium_suite_results_oct_2025)
- **Temporary result directories are gitignored** (results-YYYY-MM-DD-HH-MM-SS pattern)

## Regenerating Datasets

### Generate all test suites:
```bash
cd ../research/benchmarks

# Generate simple tests (9 instances)
./generate_simple_tests.py

# Generate medium tests (53 instances)
./generate_medium_tests.py

# Create small competition subset (8 instances)
./create_small_test_set.py
```

### Download competition benchmarks:
See `sat_competition2025/README.md` for download instructions.

## Running Benchmarks

```bash
cd ../research/benchmarks

# Run on medium tests (recommended)
./run_all_benchmarks.py ../../dataset/medium_tests -t 30

# Run on simple tests (quick validation)
./run_all_benchmarks.py ../../dataset/simple_tests -t 10

# Run on competition benchmarks (expect timeouts)
./run_all_benchmarks.py ../../dataset/sat_competition2025 -t 120
```

See `../research/benchmarks/README.md` for complete benchmarking documentation.
